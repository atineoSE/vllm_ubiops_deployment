{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "\n",
    "import ubiops\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def fetch_model(context):\n",
    "    # Loging to Hugging Face for gated models\n",
    "    login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "    # Taken from https://ubiops.com/docs/howto/howto-download-from-external-website/\n",
    "    configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "    configuration.api_key[\"Authorization\"] = os.environ[\"UBIOPS_API_TOKEN\"]\n",
    "    client = ubiops.ApiClient(configuration)\n",
    "    # api_client = ubiops.CoreApi(client)\n",
    "    project_name = context[\"project\"]\n",
    "    model_name = \"mistral-7b-instruct\"\n",
    "    model_hub_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    # model_local_path = \"./mistral-7b-instruct/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873\"\n",
    "\n",
    "    try:\n",
    "        # Retrieve from default bucket, if it exists\n",
    "        ubiops.utils.download_file(\n",
    "            client,\n",
    "            project_name,\n",
    "            bucket_name=\"default\",\n",
    "            file_name=f\"{model_name}.zip\",\n",
    "            output_path=\".\",\n",
    "            stream=True,\n",
    "            chunk_size=8192,\n",
    "        )\n",
    "        shutil.unpack_archive(f\"{model_name}.zip\", f\"./{model_name}\", \"zip\")\n",
    "        print(\"Model file loaded from object storage\")\n",
    "    except Exception as e:\n",
    "        # Fetch from Hugging Face Hub, and store to bucket for reuse, if it doesn't exist\n",
    "        print(e)\n",
    "        print(\"Model does not exist. Downloading from Hugging Face\")\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_hub_path)\n",
    "        model.save_pretrained(f\"./{model_name}\")\n",
    "\n",
    "        print(\"Storing model on UbiOps\")\n",
    "        _ = shutil.make_archive(model_name, \"zip\", model_name)\n",
    "        ubiops.utils.upload_file(client, project_name, f\"{model_name}.zip\", \"default\")\n",
    "\n",
    "    return model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Running on {num_gpus} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM as vLLM\n",
    "\n",
    "model_name = \"mistral-7b-instruct\"\n",
    "model = vLLM(model=f\"./{model_name}\", tensor_parallel_size=num_gpus)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
