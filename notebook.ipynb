{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "\n",
    "import ubiops\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def fetch_model(context):\n",
    "    # Loging to Hugging Face for gated models\n",
    "    login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "    # Taken from https://ubiops.com/docs/howto/howto-download-from-external-website/\n",
    "    configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "    configuration.api_key[\"Authorization\"] = os.environ[\"UBIOPS_API_TOKEN\"]\n",
    "    client = ubiops.ApiClient(configuration)\n",
    "    # api_client = ubiops.CoreApi(client)\n",
    "    project_name = context[\"project\"]\n",
    "    model_name = \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "    # Retrieve from default bucket (it must have been copied previously)\n",
    "    print(\"Retrieving zipped model from default bucket...\")\n",
    "    ubiops.utils.download_file(\n",
    "        client,\n",
    "        project_name,\n",
    "        bucket_name=\"default\",\n",
    "        file_name=f\"{model_name}.zip\",\n",
    "        output_path=\".\",\n",
    "        stream=True,\n",
    "        chunk_size=8192,\n",
    "    )\n",
    "    print(\"Unpacking zipped model...\")\n",
    "    shutil.unpack_archive(f\"{model_name}.zip\", f\".\", \"zip\")\n",
    "\n",
    "    print(f\"Model successfully installed to local folder {model_name}\")\n",
    "\n",
    "    return model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = fetch_model({\"project\": \"ubiops-tour\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Running on {num_gpus} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM as vLLM\n",
    "\n",
    "model_name = \"Mistral-7B-Instruct-v0.2\"\n",
    "model = vLLM(model=f\"./{model_name}\", tensor_parallel_size=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8, max_tokens=1000\n",
    ")\n",
    "response = model.generate(\n",
    "    \"Translate the following statement into Spanish: \\\"The boy fed the dog and he immediately became his friend.\\\"\",\n",
    "    sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
